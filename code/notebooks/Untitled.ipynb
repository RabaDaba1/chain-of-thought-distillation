{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5d2b1-573f-4819-9cb1-507164d6b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abb063-ba36-4a52-ab10-5d38929d815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-3B\"\n",
    "LABEL_ONLY_DIR = \"<PATH_TO_LABEL_ONLY_MODEL_DIR>\"  # TODO: set this\n",
    "SCTOD_DIR = \"<PATH_TO_SCTOD_MODEL_DIR>\"  # e.g., output_dir you saved best checkpoint to\n",
    "\n",
    "bf16 = True\n",
    "MAX_NEW_TOKENS = 256\n",
    "GEN_KW = dict(do_sample=False, temperature=0.0, top_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7141d69-d388-4743-a6db-e0824aa38dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_cot(question: str) -> str:\n",
    "    # Reuse your training prompt (system + user)\n",
    "    sys_txt = TEACHER_SYSTEM_PROMPT.strip()\n",
    "    usr_txt = TEACHER_USER_PROMPT.strip().format(question=question.strip())\n",
    "    return f\"{sys_txt}\\n\\n{usr_txt}\\n\"\n",
    "\n",
    "def build_prompt_answer_only(question: str) -> str:\n",
    "    # Minimal, answer-only prompt (for label-only SFT baseline)\n",
    "    return (\n",
    "        \"You are a helpful math assistant. Solve the problem and provide only the final numeric answer \"\n",
    "        \"in the format '#### <number>'.\\n\\n\"\n",
    "        f\"Problem: {question.strip()}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6084a-8d85-4163-a749-15d24beea8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_number(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"####\\s*(-?\\d[\\d,]*\\.?\\d*)\\b\", text)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "    nums = re.findall(r\"-?\\d[\\d,]*\\.?\\d*\", text)\n",
    "    if nums:\n",
    "        return nums[-1].replace(\",\", \"\")\n",
    "    return None\n",
    "\n",
    "def normalize_number(x: str) -> str:\n",
    "    x = x.replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        if re.fullmatch(r\"-?\\d+\", x):\n",
    "            return str(int(x))\n",
    "        v = float(x)\n",
    "        if v.is_integer():\n",
    "            return str(int(v))\n",
    "        return (\"%f\" % v).rstrip(\"0\").rstrip(\".\")\n",
    "    except Exception:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53aefc-e90e-492e-8125-bef9f6514b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(path_or_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(path_or_id, use_fast=True, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path_or_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2dee2-110e-449f-bd6f-3e84aba01c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **GEN_KW,\n",
    "        )\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        return text[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816848f-29c4-4cf0-91eb-c47e905e88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_split_greedy(model, tokenizer, dataset, prompt_builder, limit: Optional[int] = None) -> Dict[str, Any]:\n",
    "    n = len(dataset) if limit is None else min(limit, len(dataset))\n",
    "    correct, total, missing = 0, 0, 0\n",
    "    for i in range(n):\n",
    "        q = dataset[i][\"question\"]\n",
    "        gold_text = dataset[i][\"answer\"]\n",
    "        gold_num = extract_final_number(gold_text)\n",
    "        gold_num = normalize_number(gold_num) if gold_num is not None else None\n",
    "\n",
    "        prompt = prompt_builder(q)\n",
    "        gen = generate_greedy(model, tokenizer, prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        pred_raw = extract_final_number(gen)\n",
    "        pred_num = normalize_number(pred_raw) if pred_raw is not None else None\n",
    "\n",
    "        if gold_num is None or pred_num is None:\n",
    "            missing += 1\n",
    "        else:\n",
    "            correct += int(pred_num == gold_num)\n",
    "            total += 1\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i+1}/{n}: acc_so_far={(correct/max(1,total)):.4f}, missing={missing}\")\n",
    "\n",
    "    accuracy = correct / max(1, total)\n",
    "    coverage = (n - missing) / n\n",
    "    return {\"n\": n, \"correct\": correct, \"total\": total, \"accuracy\": accuracy, \"missing\": missing, \"coverage\": coverage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cade6-c917-4b57-b6cd-922ae135a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_val = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[-1000:]\")\n",
    "gsm8k_test = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6dac8-b64f-4f38-add9-8f03bf66fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Base model...\")\n",
    "base_model, base_tok = load_model_and_tokenizer(BASE_MODEL_ID)\n",
    "\n",
    "print(\"Loading Label-only SFT model...\")\n",
    "label_model, label_tok = load_model_and_tokenizer(LABEL_ONLY_DIR)\n",
    "\n",
    "print(\"Loading SCoTD student model...\")\n",
    "sctod_model, sctod_tok = load_model_and_tokenizer(SCTOD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a69420-caa5-44b6-9486-f296f30f396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = None\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating Base (prompt-only, CoT prompt)...\")\n",
    "res = evaluate_split_greedy(base_model, base_tok, gsm8k_val, build_prompt_cot, limit=LIMIT)\n",
    "results.append((\"Base Qwen2.5-3B\", \"val\", res))\n",
    "res = evaluate_split_greedy(base_model, base_tok, gsm8k_test, build_prompt_cot, limit=LIMIT)\n",
    "results.append((\"Base Qwen2.5-3B\", \"test\", res))\n",
    "\n",
    "print(\"\\nEvaluating Label-only SFT (answer-only prompt)...\")\n",
    "res = evaluate_split_greedy(label_model, label_tok, gsm8k_val, build_prompt_answer_only, limit=LIMIT)\n",
    "results.append((\"Label-only SFT\", \"val\", res))\n",
    "res = evaluate_split_greedy(label_model, label_tok, gsm8k_test, build_prompt_answer_only, limit=LIMIT)\n",
    "results.append((\"Label-only SFT\", \"test\", res))\n",
    "\n",
    "print(\"\\nEvaluating SCoTD student (CoT prompt)...\")\n",
    "res = evaluate_split_greedy(sctod_model, sctod_tok, gsm8k_val, build_prompt_cot, limit=LIMIT)\n",
    "results.append((\"SCoTD student\", \"val\", res))\n",
    "res = evaluate_split_greedy(sctod_model, sctod_tok, gsm8k_test, build_prompt_cot, limit=LIMIT)\n",
    "results.append((\"SCoTD student\", \"test\", res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f254dbe-6a65-4cd0-943b-9f88dc72e470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6130da862941998f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6130da862941998f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nResults (greedy decoding only)\")\n",
    "print(f\"{'Model':<18} {'Split':<6} {'Acc':>7} {'Coverage':>10} {'n':>6} {'correct':>8} {'total':>6} {'missing':>8}\")\n",
    "for name, split, r in results:\n",
    "    print(f\"{name:<18} {split:<6} {r['accuracy']:.4f} {r['coverage']:.4f} {r['n']:6d} {r['correct']:8d} {r['total']:6d} {r['missing']:8d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
