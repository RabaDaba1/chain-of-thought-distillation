{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "sys.path.append(str(Path.cwd().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import (\n",
    "    GSM8K_PATH,\n",
    "    TEACHER_SYSTEM_PROMPT,\n",
    "    TEACHER_USER_PROMPT,\n",
    ")\n",
    "from src.dataset_generator.helpers.answers import (\n",
    "    ParsingError,\n",
    "    parse_gold_answer_number,\n",
    "    parse_teacher_final_answer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0172f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_cot(question: str) -> str:\n",
    "    sys_txt = TEACHER_SYSTEM_PROMPT.strip()\n",
    "    usr_txt = TEACHER_USER_PROMPT.strip().format(question=question.strip())\n",
    "    return f\"{sys_txt}\\n\\n{usr_txt}\\n\"\n",
    "\n",
    "\n",
    "def build_prompt_label_only(question: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are a concise math solver. Output only the final line as:\\n\"\n",
    "        \"Final Answer: <number>\\n\\n\"\n",
    "        \"Question: A farm has 3 barns with 12 cows each. It sells 7 cows and buys 5 more. How many cows now?\\n\",\n",
    "        \"Final Answer: 34\\n\",\n",
    "        \"Pens cost $2 and notebooks $5. Alex buys 3 pens and 2 notebooks and pays with $20. How much change?\",\n",
    "        \"4\\n\",\n",
    "        \"A tank holds 250 liters. 35% is drained, then 40 liters are added. How many liters now?\",\n",
    "        \"202.5\\n\\n\",\n",
    "        f\"Question: {question.strip()}\\n\",\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b79f2-7aa3-4f3e-9e18-a0716fef4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    load_path: Optional[str] = None,\n",
    "    bf16: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    ") -> Tuple[Any, Any]:\n",
    "    quant_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "\n",
    "    if load_path is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            load_path,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        load_path if load_path else model_id, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c487241-9b59-4b6a-9962-cd4e05698aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    questions: list[str],\n",
    "    mode: str,\n",
    "    progress_desc: str,\n",
    "    max_new_tokens: int,\n",
    "    batch_size: int,\n",
    ") -> list[str]:\n",
    "    build_prompt = build_prompt_cot if mode == \"cot\" else build_prompt_label_only\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=progress_desc):\n",
    "            chunk = questions[i : i + batch_size]\n",
    "            prompts = [build_prompt(q) for q in chunk]\n",
    "            enc = tokenizer(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,  # safe cap; adjust if needed\n",
    "            )\n",
    "            # Send to model device\n",
    "            enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "            gen = model.generate(\n",
    "                **enc,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            texts = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "            # Slice off the prompt\n",
    "            for prompt, full in zip(prompts, texts):\n",
    "                outputs.append(full[len(prompt) :].strip())\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_gsm8k_greedy_batched(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    mode: str,\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    split: str = \"test\",\n",
    "    limit: Optional[int] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    mode: \"cot\" or \"label-only\"\n",
    "    \"\"\"\n",
    "    ds = load_dataset(GSM8K_PATH, \"main\", split=split)\n",
    "\n",
    "    if limit is not None:\n",
    "        ds = ds.select(range(min(limit, len(ds))))\n",
    "\n",
    "    questions = ds[\"question\"]\n",
    "    gold_texts = ds[\"answer\"]\n",
    "    gold_nums = [parse_gold_answer_number(t) for t in gold_texts]\n",
    "\n",
    "    gens = batch_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        questions,\n",
    "        mode=mode,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        batch_size=batch_size,\n",
    "        progress_desc=f\"Evaluating ({mode}, greedy, bs={batch_size})\",\n",
    "    )\n",
    "\n",
    "    n_total = len(gens)\n",
    "    n_correct = 0\n",
    "    for pred_text, gold_num in zip(gens, gold_nums):\n",
    "        try:\n",
    "            pred_num = parse_teacher_final_answer(pred_text)\n",
    "        except ParsingError:\n",
    "            pred_num = None\n",
    "        except Exception:\n",
    "            pred_num = None\n",
    "\n",
    "        if (\n",
    "            pred_num is not None\n",
    "            and gold_num is not None\n",
    "            and numbers_equal(pred_num, gold_num)\n",
    "        ):\n",
    "            n_correct += 1\n",
    "\n",
    "    return {\"accuracy\": n_correct / n_total if n_total else 0.0, \"n\": n_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a510dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce16627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading student_sctod (adapter/merged) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/chain-of-thought-distillation/code/.venv/lib/python3.11/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Evaluating (cot, greedy, bs=16): 100%|██████████| 83/83 [22:35<00:00, 16.33s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numbers_equal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     32\u001b[39m     model.config.use_cache = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m metrics = \u001b[43mevaluate_gsm8k_greedy_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m results.append((name, metrics))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mevaluate_gsm8k_greedy_batched\u001b[39m\u001b[34m(model, tokenizer, mode, split, limit, batch_size, max_new_tokens)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     82\u001b[39m         pred_num = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pred_num \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gold_num \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mnumbers_equal\u001b[49m(pred_num, gold_num):\n\u001b[32m     85\u001b[39m         n_correct += \u001b[32m1\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: n_correct / n_total \u001b[38;5;28;01mif\u001b[39;00m n_total \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n_total}\n",
      "\u001b[31mNameError\u001b[39m: name 'numbers_equal' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-3B\"\n",
    "SCTOD_PATH = \"../artifacts/models/qwen2.5_3b_sctod_lora/best_checkpoint\"\n",
    "LABELONLY_PATH = \"../artifacts/models/qwen2.5_3b_labelonly_lora/best_checkpoint\"\n",
    "\n",
    "RUNS = [\n",
    "    {\"name\": \"student_sctod\", \"mode\": \"cot\", \"path\": SCTOD_PATH},\n",
    "    {\"name\": \"student_label_only\", \"mode\": \"label-only\", \"path\": LABELONLY_PATH},\n",
    "    {\"name\": \"base_cot_prompting\", \"mode\": \"cot\", \"path\": None},\n",
    "    {\"name\": \"base_label_only\", \"mode\": \"label-only\", \"path\": None},\n",
    "]\n",
    "\n",
    "limit = None\n",
    "batch_size = 16\n",
    "max_new_tokens = 256\n",
    "\n",
    "results = []\n",
    "for run in RUNS:\n",
    "    name = run[\"name\"]\n",
    "    mode = run[\"mode\"]\n",
    "    path = run[\"path\"]\n",
    "    print(f\"\\n=== Loading {name} ({'adapter/merged' if path else 'base'}) ===\")\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        model_id=MODEL_ID,\n",
    "        load_path=path,\n",
    "        bf16=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # Ensure eval-time cache is on (may have been disabled in training config)\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.use_cache = True\n",
    "\n",
    "    metrics = evaluate_gsm8k_greedy_batched(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        mode=mode,\n",
    "        split=\"test\",\n",
    "        limit=limit,\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    print(f\"{name} -> accuracy: {metrics['accuracy']:.4f} (n={metrics['n']})\")\n",
    "    results.append((name, metrics))\n",
    "\n",
    "print(\"\\n=== Summary (greedy only) ===\")\n",
    "for name, m in results:\n",
    "    print(f\"{name:>24}: {m['accuracy']:.4f} (n={m['n']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4093de-3eb4-4529-92c5-16e5938eef81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1822d-e957-435f-884a-da80baa5e2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv .venv)",
   "language": "python",
   "name": "uv-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
