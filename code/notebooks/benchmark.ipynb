{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f9c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "sys.path.append(str(Path.cwd().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import (\n",
    "    GSM8K_PATH,\n",
    "    TEACHER_SYSTEM_PROMPT,\n",
    "    TEACHER_USER_PROMPT,\n",
    ")\n",
    "from src.dataset_generator.helpers.answers import (\n",
    "    ParsingError,\n",
    "    parse_gold_answer_number,\n",
    "    parse_teacher_final_answer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0172f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_cot(question: str) -> str:\n",
    "    sys_txt = TEACHER_SYSTEM_PROMPT.strip()\n",
    "    usr_txt = TEACHER_USER_PROMPT.strip().format(question=question.strip())\n",
    "    return f\"{sys_txt}\\n\\n{usr_txt}\\n\"\n",
    "\n",
    "\n",
    "def build_prompt_label_only(question: str) -> str:\n",
    "    shots = [\n",
    "        (\n",
    "            \"A farm has 3 barns with 12 cows each. It sells 7 cows and buys 5 more. How many cows now?\",\n",
    "            \"34\",\n",
    "        ),\n",
    "        (\n",
    "            \"Pens cost $2 and notebooks $5. Alex buys 3 pens and 2 notebooks and pays with $20. How much change?\",\n",
    "            \"4\",\n",
    "        ),\n",
    "        (\n",
    "            \"A tank holds 250 liters. 35% is drained, then 40 liters are added. How many liters now?\",\n",
    "            \"202.5\",\n",
    "        ),\n",
    "    ]\n",
    "    header = (\n",
    "        \"You are a concise math solver. Output only the final line as:\\n\"\n",
    "        \"Final Answer: <number>\\n\\n\"\n",
    "    )\n",
    "    exemplars = [f\"Question: {q}\\nFinal Answer: {a}\" for q, a in shots]\n",
    "    exemplars_txt = \"\\n\\n\".join(exemplars)\n",
    "    return f\"{header}{exemplars_txt}\\n\\nQuestion: {question.strip()}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36b79f2-7aa3-4f3e-9e18-a0716fef4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    peft_or_merged_path: Optional[str] = None,\n",
    "    use_4bit: bool = True,\n",
    "    bf16: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Loads either:\n",
    "      - base model only (when peft_or_merged_path=None)\n",
    "      - base+adapter (when peft_or_merged_path points to a PEFT dir with adapter_config.json)\n",
    "      - merged model (when peft_or_merged_path points to a standard HF model dir)\n",
    "    Returns (model, tokenizer)\n",
    "    \"\"\"\n",
    "    quant_cfg = None\n",
    "    if use_4bit:\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "\n",
    "    load_path = peft_or_merged_path\n",
    "\n",
    "    if load_path is None:\n",
    "        # Base model only\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tok_src = model_id\n",
    "    else:\n",
    "        # Base + adapter\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            load_path,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tok_src = load_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tok_src, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c487241-9b59-4b6a-9962-cd4e05698aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    questions: list[str],\n",
    "    mode: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    batch_size: int = 16,\n",
    "    progress_desc: str = \"\",\n",
    ") -> list[str]:\n",
    "    build_prompt = build_prompt_cot if mode == \"cot\" else build_prompt_label_only\n",
    "    outputs: List[str] = []\n",
    "    model.eval()\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=progress_desc):\n",
    "            chunk = questions[i:i+batch_size]\n",
    "            prompts = [build_prompt(q) for q in chunk]\n",
    "            enc = tokenizer(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,  # safe cap; adjust if needed\n",
    "            )\n",
    "            # Send to model device\n",
    "            enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "            gen = model.generate(\n",
    "                **enc,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            texts = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "            # Slice off the prompt\n",
    "            for prompt, full in zip(prompts, texts):\n",
    "                outputs.append(full[len(prompt):].strip())\n",
    "    return outputs\n",
    "\n",
    "def evaluate_gsm8k_greedy_batched(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    mode: str,\n",
    "    split: str = \"test\",\n",
    "    limit: Optional[int] = None,\n",
    "    batch_size: int = 16,\n",
    "    max_new_tokens: int = 256,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    mode: \"cot\" or \"label-only\"\n",
    "    \"\"\"\n",
    "    ds = load_dataset(GSM8K_PATH, \"main\", split=split)\n",
    "\n",
    "    if limit is not None:\n",
    "        ds = ds.select(range(min(limit, len(ds))))\n",
    "\n",
    "    questions = ds[\"question\"]\n",
    "    gold_texts = ds[\"answer\"]\n",
    "    gold_nums = [parse_gold_answer_number(t) for t in gold_texts]\n",
    "\n",
    "    gens = batch_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        questions,\n",
    "        mode=mode,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        batch_size=batch_size,\n",
    "        progress_desc=f\"Evaluating ({mode}, greedy, bs={batch_size})\",\n",
    "    )\n",
    "\n",
    "    n_total = len(gens)\n",
    "    n_correct = 0\n",
    "    for pred_text, gold_num in zip(gens, gold_nums):\n",
    "        try:\n",
    "            pred_num = parse_teacher_final_answer(pred_text)\n",
    "        except ParsingError:\n",
    "            pred_num = None\n",
    "        except Exception:\n",
    "            pred_num = None\n",
    "\n",
    "        if pred_num is not None and gold_num is not None and pred_num == gold_num:\n",
    "            n_correct += 1\n",
    "\n",
    "    return {\"accuracy\": n_correct / n_total if n_total else 0.0, \"n\": n_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a510dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce16627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading student_sctod (adapter/merged) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (cot, greedy, bs=16):   5%|‚ñç         | 4/83 [00:58<17:50, 13.54s/it]"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-3B\"\n",
    "# Point these to your checkpoints. If directory contains adapter_config.json -> treated as adapter (PEFT).\n",
    "SCTOD_PATH = \"../artifacts/models/qwen2.5_3b_sctod_lora/best_checkpoint\"\n",
    "LABELONLY_PATH = \"../artifacts/models/qwen2.5_3b_labelonly_lora/best_checkpoint\"\n",
    "\n",
    "RUNS = [\n",
    "    {\"name\": \"student_sctod\", \"mode\": \"cot\", \"path\": SCTOD_PATH},\n",
    "    {\"name\": \"student_label_only\", \"mode\": \"label-only\", \"path\": LABELONLY_PATH},\n",
    "    {\"name\": \"base_cot_prompting\", \"mode\": \"cot\", \"path\": None},\n",
    "    {\"name\": \"base_label_only\", \"mode\": \"label-only\", \"path\": None},\n",
    "]\n",
    "\n",
    "limit = None   # e.g., 100 for a quick smoke test\n",
    "batch_size = 16\n",
    "max_new_tokens = 256\n",
    "\n",
    "results = []\n",
    "for run in RUNS:\n",
    "    name = run[\"name\"]\n",
    "    mode = run[\"mode\"]\n",
    "    path = run[\"path\"]\n",
    "    print(f\"\\n=== Loading {name} ({'adapter/merged' if path else 'base'}) ===\")\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        model_id=MODEL_ID,\n",
    "        peft_or_merged_path=path,\n",
    "        use_4bit=True,\n",
    "        bf16=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # Ensure eval-time cache is on (may have been disabled in training config)\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.use_cache = True\n",
    "\n",
    "    metrics = evaluate_gsm8k_greedy_batched(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        mode=mode,\n",
    "        split=\"test\",\n",
    "        limit=limit,\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    print(f\"{name} -> accuracy: {metrics['accuracy']:.4f} (n={metrics['n']})\")\n",
    "    results.append((name, metrics))\n",
    "\n",
    "print(\"\\n=== Summary (greedy only) ===\")\n",
    "for name, m in results:\n",
    "    print(f\"{name:>24}: {m['accuracy']:.4f} (n={m['n']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4093de-3eb4-4529-92c5-16e5938eef81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1822d-e957-435f-884a-da80baa5e2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv .venv)",
   "language": "python",
   "name": "uv-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
