{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "sys.path.append(str(Path.cwd().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import (\n",
    "    GSM8K_PATH,\n",
    "    TEACHER_SYSTEM_PROMPT,\n",
    "    TEACHER_USER_PROMPT,\n",
    ")\n",
    "from src.dataset_generator.helpers.answers import (\n",
    "    ParsingError,\n",
    "    parse_gold_answer_number,\n",
    "    parse_teacher_final_answer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0172f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_cot(question: str) -> str:\n",
    "    sys_txt = TEACHER_SYSTEM_PROMPT.strip()\n",
    "    usr_txt = TEACHER_USER_PROMPT.strip().format(question=question.strip())\n",
    "    return f\"{sys_txt}\\n\\n{usr_txt}\\n\"\n",
    "\n",
    "\n",
    "def build_prompt_label_only(question: str) -> str:\n",
    "    shots = [\n",
    "        (\n",
    "            \"A farm has 3 barns with 12 cows each. It sells 7 cows and buys 5 more. How many cows now?\",\n",
    "            \"34\",\n",
    "        ),\n",
    "        (\n",
    "            \"Pens cost $2 and notebooks $5. Alex buys 3 pens and 2 notebooks and pays with $20. How much change?\",\n",
    "            \"4\",\n",
    "        ),\n",
    "        (\n",
    "            \"A tank holds 250 liters. 35% is drained, then 40 liters are added. How many liters now?\",\n",
    "            \"202.5\",\n",
    "        ),\n",
    "    ]\n",
    "    header = (\n",
    "        \"You are a concise math solver. Output only the final line as:\\n\"\n",
    "        \"Final Answer: <number>\\n\\n\"\n",
    "    )\n",
    "    exemplars = [f\"Question: {q}\\nFinal Answer: {a}\" for q, a in shots]\n",
    "    exemplars_txt = \"\\n\\n\".join(exemplars)\n",
    "    return f\"{header}{exemplars_txt}\\n\\nQuestion: {question.strip()}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    peft_or_merged_path: Optional[str] = None,\n",
    "    use_4bit: bool = True,\n",
    "    bf16: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Loads either:\n",
    "      - base model only (when peft_or_merged_path=None)\n",
    "      - base+adapter (when peft_or_merged_path points to a PEFT dir with adapter_config.json)\n",
    "      - merged model (when peft_or_merged_path points to a standard HF model dir)\n",
    "    Returns (model, tokenizer)\n",
    "    \"\"\"\n",
    "    quant_cfg = None\n",
    "    if use_4bit:\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "\n",
    "    load_path = peft_or_merged_path\n",
    "\n",
    "    if load_path is None:\n",
    "        # Base model only\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tok_src = model_id\n",
    "    else:\n",
    "        # Base + adapter\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base, load_path, is_trainable=False)\n",
    "        tok_src = model_id\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tok_src, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    model, tokenizer, mode: str, question: str, max_new_tokens: int = 256\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        build_prompt = build_prompt_cot if mode == \"cot\" else build_prompt_label_only\n",
    "        prompt = build_prompt(question)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        return text[len(prompt) :].strip()\n",
    "\n",
    "\n",
    "def evaluate_gsm8k_greedy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    mode: str,\n",
    "    split: str = \"test\",\n",
    "    limit: Optional[int] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    mode: \"cot\" or \"label-only\"\n",
    "    Returns metrics dict with 'accuracy' and 'n'\n",
    "    \"\"\"\n",
    "    ds = load_dataset(GSM8K_PATH, \"main\", split=split)\n",
    "\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "    it = ds if limit is None else ds.select(range(min(limit, len(ds))))\n",
    "\n",
    "    for ex in tqdm(it, desc=f\"Evaluating ({mode}, greedy)\"):\n",
    "        q = ex[\"question\"]\n",
    "        gold_text = ex[\"answer\"]\n",
    "        gold_num = parse_gold_answer_number(gold_text)\n",
    "\n",
    "        try:\n",
    "            gen = generate_answer(model, tokenizer, mode, q)\n",
    "            pred_num = parse_teacher_final_answer(gen)\n",
    "        except ParsingError:\n",
    "            pred_num = None\n",
    "        except Exception:\n",
    "            pred_num = None\n",
    "\n",
    "        if pred_num is not None and gold_num is not None and pred_num == gold_num:\n",
    "            n_correct += 1\n",
    "        n_total += 1\n",
    "\n",
    "    return {\"accuracy\": n_correct / n_total if n_total else 0.0, \"n\": n_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bf16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce16627",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "SCTOD_PATH = \"/path/to/models/qwen2.5_3b_sctod_lora/best_checkpoint\"\n",
    "LABELONLY_PATH = \"/path/to/models/qwen2.5_3b_labelonly_lora/best_checkpoint\"\n",
    "\n",
    "RUNS = [\n",
    "    {\"name\": \"student_sctod\", \"mode\": \"cot\", \"path\": SCTOD_PATH},\n",
    "    {\"name\": \"student_label_only\", \"mode\": \"label-only\", \"path\": LABELONLY_PATH},\n",
    "    {\"name\": \"base_cot_prompting\", \"mode\": \"cot\", \"path\": None},\n",
    "    {\"name\": \"base_label_only\", \"mode\": \"label-only\", \"path\": None},\n",
    "]\n",
    "\n",
    "\n",
    "def main(limit: Optional[int] = None):\n",
    "    results = []\n",
    "    for run in RUNS:\n",
    "        name = run[\"name\"]\n",
    "        mode = run[\"mode\"]\n",
    "        path = run[\"path\"]\n",
    "\n",
    "        print(f\"\\n=== Loading {name} ({'adapter/merged' if path else 'base'}) ===\")\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=MODEL_ID,\n",
    "            peft_or_merged_path=path,\n",
    "            use_4bit=True,\n",
    "            bf16=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        metrics = evaluate_gsm8k_greedy(\n",
    "            model, tokenizer, mode=mode, split=\"test\", limit=limit\n",
    "        )\n",
    "        print(f\"{name} -> accuracy: {metrics['accuracy']:.4f} (n={metrics['n']})\")\n",
    "        results.append((name, metrics))\n",
    "\n",
    "    print(\"\\n=== Summary (greedy only) ===\")\n",
    "    for name, m in results:\n",
    "        print(f\"{name:>24}: {m['accuracy']:.4f} (n={m['n']})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(limit=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engineering-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
