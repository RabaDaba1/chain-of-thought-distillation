{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f9c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "sys.path.append(str(Path.cwd().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import (\n",
    "    GSM8K_PATH,\n",
    "    TEACHER_SYSTEM_PROMPT,\n",
    "    TEACHER_USER_PROMPT,\n",
    ")\n",
    "from src.dataset_generator.helpers.answers import (\n",
    "    ParsingError,\n",
    "    parse_gold_answer_number,\n",
    "    parse_teacher_final_answer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0172f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_cot(question: str) -> str:\n",
    "    sys_txt = TEACHER_SYSTEM_PROMPT.strip()\n",
    "    usr_txt = TEACHER_USER_PROMPT.strip().format(question=question.strip())\n",
    "    return f\"{sys_txt}\\n\\n{usr_txt}\\n\"\n",
    "\n",
    "\n",
    "def build_prompt_label_only(question: str) -> str:\n",
    "    shots = [\n",
    "        (\n",
    "            \"A farm has 3 barns with 12 cows each. It sells 7 cows and buys 5 more. How many cows now?\",\n",
    "            \"34\",\n",
    "        ),\n",
    "        (\n",
    "            \"Pens cost $2 and notebooks $5. Alex buys 3 pens and 2 notebooks and pays with $20. How much change?\",\n",
    "            \"4\",\n",
    "        ),\n",
    "        (\n",
    "            \"A tank holds 250 liters. 35% is drained, then 40 liters are added. How many liters now?\",\n",
    "            \"202.5\",\n",
    "        ),\n",
    "    ]\n",
    "    header = (\n",
    "        \"You are a concise math solver. Output only the final line as:\\n\"\n",
    "        \"Final Answer: <number>\\n\\n\"\n",
    "    )\n",
    "    exemplars = [f\"Question: {q}\\nFinal Answer: {a}\" for q, a in shots]\n",
    "    exemplars_txt = \"\\n\\n\".join(exemplars)\n",
    "    return f\"{header}{exemplars_txt}\\n\\nQuestion: {question.strip()}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdceb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    peft_or_merged_path: Optional[str] = None,\n",
    "    use_4bit: bool = True,\n",
    "    bf16: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Loads either:\n",
    "      - base model only (when peft_or_merged_path=None)\n",
    "      - base+adapter (when peft_or_merged_path points to a PEFT dir with adapter_config.json)\n",
    "      - merged model (when peft_or_merged_path points to a standard HF model dir)\n",
    "    Returns (model, tokenizer)\n",
    "    \"\"\"\n",
    "    quant_cfg = None\n",
    "    if use_4bit:\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "\n",
    "    load_path = peft_or_merged_path\n",
    "\n",
    "    if load_path is None:\n",
    "        # Base model only\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tok_src = model_id\n",
    "    else:\n",
    "        # Base + adapter\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            load_path,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tok_src = load_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tok_src, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    model, tokenizer, mode: str, question: str, max_new_tokens: int = 256\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        build_prompt = build_prompt_cot if mode == \"cot\" else build_prompt_label_only\n",
    "        prompt = build_prompt(question)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        return text[len(prompt) :].strip()\n",
    "\n",
    "\n",
    "def evaluate_gsm8k_greedy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    mode: str,\n",
    "    split: str = \"test\",\n",
    "    limit: Optional[int] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    mode: \"cot\" or \"label-only\"\n",
    "    Returns metrics dict with 'accuracy' and 'n'\n",
    "    \"\"\"\n",
    "    ds = load_dataset(GSM8K_PATH, \"main\", split=split)\n",
    "\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "    it = ds if limit is None else ds.select(range(min(limit, len(ds))))\n",
    "\n",
    "    for ex in tqdm(it, desc=f\"Evaluating ({mode}, greedy)\"):\n",
    "        q = ex[\"question\"]\n",
    "        gold_text = ex[\"answer\"]\n",
    "        gold_num = parse_gold_answer_number(gold_text)\n",
    "\n",
    "        try:\n",
    "            gen = generate_answer(model, tokenizer, mode, q)\n",
    "            pred_num = parse_teacher_final_answer(gen)\n",
    "        except ParsingError:\n",
    "            pred_num = None\n",
    "        except Exception:\n",
    "            pred_num = None\n",
    "\n",
    "        if pred_num is not None and gold_num is not None and pred_num == gold_num:\n",
    "            n_correct += 1\n",
    "        n_total += 1\n",
    "\n",
    "    return {\"accuracy\": n_correct / n_total if n_total else 0.0, \"n\": n_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a510dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bf16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce16627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading student_sctod (adapter/merged) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/chain-of-thought-distillation/code/.venv/lib/python3.11/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12cb2b119a4405c8cf3c7010a956693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d601c7dedfc0475b9edbe3aa0c809886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccc328b6be340a99c7c4c8da2425ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a3de0f8baa4024a4bf7c3d65a9c47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6913df95163649578e27e72203e0ee57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (cot, greedy):   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   1%|          | 1/100 [00:05<09:46,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   2%|▏         | 2/100 [00:08<06:35,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   3%|▎         | 3/100 [00:17<10:26,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   4%|▍         | 4/100 [00:21<08:47,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   5%|▌         | 5/100 [00:26<08:07,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   6%|▌         | 6/100 [00:42<13:53,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   7%|▋         | 7/100 [00:46<11:11,  7.22s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   8%|▊         | 8/100 [00:54<11:27,  7.47s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):   9%|▉         | 9/100 [01:02<11:40,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  10%|█         | 10/100 [01:08<10:45,  7.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  11%|█         | 11/100 [01:14<09:56,  6.71s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  12%|█▏        | 12/100 [01:19<09:06,  6.21s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  13%|█▎        | 13/100 [01:26<09:25,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  14%|█▍        | 14/100 [01:38<11:40,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  15%|█▌        | 15/100 [01:46<11:18,  7.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Evaluating (cot, greedy):  16%|█▌        | 16/100 [01:52<10:22,  7.41s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "SCTOD_PATH = \"../artifacts/models/qwen2.5_3b_sctod_lora/best_checkpoint\"\n",
    "LABELONLY_PATH = \"../artifacts/models/qwen2.5_3b_labelonly_lora/best_checkpoint\"\n",
    "\n",
    "RUNS = [\n",
    "    {\"name\": \"student_sctod\", \"mode\": \"cot\", \"path\": SCTOD_PATH},\n",
    "    {\"name\": \"student_label_only\", \"mode\": \"label-only\", \"path\": LABELONLY_PATH},\n",
    "    {\"name\": \"base_cot_prompting\", \"mode\": \"cot\", \"path\": None},\n",
    "    {\"name\": \"base_label_only\", \"mode\": \"label-only\", \"path\": None},\n",
    "]\n",
    "\n",
    "\n",
    "def main(limit: Optional[int] = None):\n",
    "    results = []\n",
    "    for run in RUNS:\n",
    "        name = run[\"name\"]\n",
    "        mode = run[\"mode\"]\n",
    "        path = run[\"path\"]\n",
    "\n",
    "        print(f\"\\n=== Loading {name} ({'adapter/merged' if path else 'base'}) ===\")\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=MODEL_ID,\n",
    "            peft_or_merged_path=path,\n",
    "            use_4bit=True,\n",
    "            bf16=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        metrics = evaluate_gsm8k_greedy(\n",
    "            model, tokenizer, mode=mode, split=\"test\", limit=limit\n",
    "        )\n",
    "        print(f\"{name} -> accuracy: {metrics['accuracy']:.4f} (n={metrics['n']})\")\n",
    "        results.append((name, metrics))\n",
    "\n",
    "    print(\"\\n=== Summary (greedy only) ===\")\n",
    "    for name, m in results:\n",
    "        print(f\"{name:>24}: {m['accuracy']:.4f} (n={m['n']})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4093de-3eb4-4529-92c5-16e5938eef81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv .venv)",
   "language": "python",
   "name": "uv-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
