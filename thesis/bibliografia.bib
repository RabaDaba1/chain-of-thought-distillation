@Book{Lam92,
author = {Lamport, L.},
title = {LaTeX system przygotowywania dokumentów},
publisher = {Wydawnictwo Ariel},
year = {1992},
address = {Kraków},

langid = {polish},
}

@Book{Dil00,
author = {Diller, A.},
title = {LaTeX wiersz po wierszu},
publisher = {Wydawnictwo Helion},
year = {2000},
address = {Gliwice},

langid = {polish},
}


@Manual{ARM06,
title = {Ada Reference Manual {ISO}/{IEC} 8652:200y({E}) {E}d. 3},
organization = {Ada Europe},
year = {2006}
}

@Article{BuDo03,
author = {Burns, A. and Dobbing, B.},
title = {The {R}avenscar {P}rofile for Real--Time and High Integrity Systems},
journal = {Cross{T}alk},
year = {2003},
volume = {16},
number = {11},
pages = {9--12}
}



@InProceedings{PeDa04,
author = {Peleska, J. and Große, D. and Haxthausen, A. E. and Drechsler, R.},
title = {Automated Verification for Train Control Systems},
booktitle = {Proc. of the 5th Symposium on Formal Methods for Automation and Safety in Railway and Automotive Systems (FORMS/FORMAT 2004)},
year = {2004},
pages = {252--265},
address = {Braunschweig, Germany},
month = {December}
}

@TechReport{BuDoVa03,
author = {Burns, A. and Dobbing, B. and Vardanega, T.},
title = {Guide for the Use of the Ada Ravenscar Profile in High Integrity Systems},
institution = {University of York},
year = {2003},
number = {YCS-2003-348}
}



@manual{Alvis2011,
	organization = {AGH University of Science and Technology},
	year = {2011},
	title = {{On Line Alvis Manual}},
	note = {\\\texttt{http://fm.ia.agh.edu.pl/alvis:manual}},
	author = {Szpyrka, M.},

	langid = {english},
}

@inproceedings{Wei2022CoT,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and others},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle = {NeurIPS},
  year      = {2022},
  note      = {arXiv:2201.11903},
  langid    = {english}
}

@article{Wang2023SelfConsistency,
  author  = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V. and Chi, Ed H. and others},
  title   = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  journal = {arXiv preprint},
  year    = {2023},
  note    = {arXiv:2203.11171},
  langid  = {english}
}

@article{Cobbe2021GSM8K,
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and others},
  title   = {Training Verifiers to Solve Math Word Problems},
  journal = {arXiv preprint},
  year    = {2021},
  note    = {arXiv:2110.14168 (introduces GSM8K)},
  langid  = {english}
}

% Zero-shot CoT
@article{Kojima2022ZeroShotCoT,
  author  = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  title   = {Large Language Models are Zero-Shot Reasoners},
  journal = {arXiv preprint},
  year    = {2022},
  note    = {arXiv:2205.11916},
  langid  = {english}
}

% Gemma model (student)
@misc{Gemma2024,
  author = {{Gemma Team}},
  title  = {Gemma: Open Models by Google},
  year   = {2024},
  note   = {arXiv:2403.08295},
  langid = {english}
}

% SCoTD placeholder — update with the exact paper details (avoid fabrication)
@misc{Li2023SCoTD_PLACEHOLDER,
  author = {Li, [FirstName] and others},
  title  = {Self-consistent Chain-of-Thought Distillation},
  year   = {2023},
  note   = {PLACEHOLDER: update authors, title, venue, and arXiv/DOI after verification},
  langid = {english}
}

% Optional: add teacher citation after confirming the exact model/paper.
% @misc{QwQ32B_PLACEHOLDER,
%   author = {[Team/Authors]},
%   title  = {QwQ-32B [Exact variant]},
%   year   = {2024},
%   note   = {PLACEHOLDER: update with official report/repo/DOI},
%   langid = {english}
% }

@inproceedings{Hinton2015Distilling,
  author    = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  title     = {Distilling the Knowledge in a Neural Network},
  booktitle = {NIPS 2014 Deep Learning and Representation Learning Workshop},
  year      = {2015},
  note      = {arXiv:1503.02531},
  langid    = {english}
}

@inproceedings{Hu2021LoRA,
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {ICLR},
  year      = {2022},
  note      = {arXiv:2106.09685},
  langid    = {english}
}

@inproceedings{Dettmers2023QLoRA,
  author    = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title     = {QLoRA: Efficient Finetuning of Quantized LLMs},
  booktitle = {NeurIPS},
  year      = {2023},
  note      = {arXiv:2305.14314},
  langid    = {english}
}

@inproceedings{Brown2020GPT3,
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {NeurIPS},
  year      = {2020},
  note      = {arXiv:2005.14165},
  langid    = {english}
}
